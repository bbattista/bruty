## Comments have two hashes at the beginning of the line
## examples (which are not the default values) have two hashes and "ex:" at the beginning of the line
## The default values are commented out with a single hash at the beginning of the line

[DEFAULT]
## additional configs to load -- separated by commas or newlines (remember to indent if newlines)
## The leftmost is the highest priority, so if there are duplicate settings, the leftmost file will be used
# ex: additional_configs = nbs_postgres.config, nbs06.config
# additional_configs =

## *required* database is the name of the database to connect to which holds the metadata records about scoring and survey file paths
# database = metadata

## *required* File paths to point to the connection info for the postgres database, probably specified in an additional config that is machine specific
## ex: URL_FILENAME = d:\subdir\postgres_hostname.txt
# URL_FILENAME =
## ex: CREDENTIALS_FILENAME = d:\subdir\my_credentials.txt
# CREDENTIALS_FILENAME =

## *required* path to the bruty combines, an example would be W:\debug\combines
# data_dir =

## *required* path to the environment to activate when running multiple processes
## ex: environment_path = C:\Pydro22_Dev\Scripts\activate
## ex: environment_name = NBS20231113
# environment_path =
# environment_name =

## zones and tiles can use ranges or specific integers, e.g.  "2, 3, 5-8"  is the same as "2,3,5,6,7,8"
## if not specified then all zones and tiles will be processed
## zones = 18, 19
# zones =
## tiles = 1,2,5-8
# tiles =

## production_branches, datums and dtypes can have multiple values separated by commas, e.g. PBB, PBG, PBC
## production_branches = PBB,PBG
## if not specified then all production_branches will be processed
# production_branches =

## options for dtypes = gmrt, enc, qualified, unqualified, sensitive
## also, prepending "+"s or "-" will raise or lower the priority of the dtype by the number of characters.
## (the priority ammount is added to the priority column in the spec_tiles table)
## The following would process qualified faster then gmrt and enc would be slower than everything else.
## ex:  dtypes = +gmrt, -enc, ++qualified, unqualified, sensitive
## The following will never process enc or sensitive
## ex:  dtypes = gmrt, qualified, unqualified
## if not specified then all dtypes will be processed
# dtypes =

## Datums to be processed, like MLLW or HRD
## if not specified then all datums will be processed
# datums =

## the number of consoles to open to run tasks in.  This should be set in the machine.config (like NBS03.config)
# processes = 5

## set if you want the windows to be minimized on the taskbar
# MINIMIZED = False
## Windows ONLY -- keeps the console open if an error is reported on exit
# ALWAYS_EXIT = False

## use the lock_server if running multiple processes on the same tile
# lock_server_port =

## options for the amount of logging, valid options are DEBUG, INFO, WARNING
# LOG_LEVEL = WARNING

[VALIDATE]
## flag to make validate.py perform repairs while running.
## If False then errors are reported but not fixed (which is faster and should finish more reliably)
# repair = False

# [COMBINE]  # settings that only affect combine
## override the combine spec table that specifies if a tile should be built
# force_build = False

## Remove the existing bruty database and start from scratch IF any records need to be cleaned.
## This is much faster than removing all entries if the nbs_ids are all changed or some other large change to existing postgres database
# delete_existing = False

## override the epsg codes in the data for combine with epsg of the bruty database
# override = True


# [EXPORT]  # settings that only affect export
## *required* path to the export tiff directory
# export_dir = W:\debug\exports

## USE_CACHED_ENC_METADATA uses stored postgres records for all dtypes, default is False
# USE_CACHED_METADATA = True

## USE_CACHED_ENC_METADATA uses stored postgres records for just ENCs, default is False
# USE_CACHED_ENC_METADATA = True

## number of decimals to store - if not specified then full resolutions is kept
## so a raw value of -34.76548 would be saved.  If decimals = 2 then -34.76 would be kept (it uses ceil)
## this is useful for compression to work better and reduce storage sizes
## ex: decimals = 2
## default is None (meaning full resolution) but NBS uses 2 for compression sizes of output tiffs
# decimals =

## The number of exports to keep when cleaning_old_exports.py runs, default is 2
# retain = 2

## Table to write records into describing the tiffs that were created by tile_export.py (or export_tiles.py)
## ex: export_database = tile_specifications
## ex: export_table = xbox
# export_database =
# export_table =

[DEFAULT]  # some debugging parameters (only DEV should use)
## res is debugging parameter only
# res = 4
# exclude_ids = 192003196, 192003942, 192000052, 192003842, 192003889, 192001508, 192000065, 192000099, 192002674, 192000138, 192002770, 192003865

## if processes is set to 1 and DEBUG is True then special code will be run in PyCharm to facilitate interactive debugging
## DEBUG = True
# DEBUG =

## perform processing on the combines and/or exports
## process_combines = True
## process_exports = True
# process_combines =
# process_exports =

## Run the script forever as a service that just keeps checking for more data
# RUN_AS_SERVICE = False

## capture to a file the console output for the subprocesses running combines/exports
## This is useful if running with no graphical output (scheduled task/service) and the subprocesses are ending before logging is started (environment problems, disk paths fail etc)
## To capture the stderr use 2>
## You can not use "2>&1"  which would normally put stderr into stdout but the & is the join character for multiple commands and will fail.
## This example would overwrite the stdout file but append to the stderr file.  You may want to append both to the same file.
## ex: console_capture = > d:\git_repos\sched_task_out.txt 2>> d:\git_repos\sched_task_err.txt
# console_capture =

